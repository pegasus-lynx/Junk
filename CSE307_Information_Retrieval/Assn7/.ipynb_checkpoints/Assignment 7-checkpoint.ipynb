{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    def __init__(self,base_url,store_dir='./store'):\n",
    "        self.url_frontier = []\n",
    "        self.base_url = base_url\n",
    "        if not os.path.exists(store_dir):\n",
    "            os.mkdir(store_dir)\n",
    "        self.store = store_dir\n",
    "    \n",
    "    def scrap(self, seed_url, num_docs=100):\n",
    "        \n",
    "        # URL Frontier \n",
    "        self.url_frontier = []\n",
    "        self.url_frontier.append(seed_url)\n",
    "        scraped_links = set()\n",
    "        \n",
    "        # Doc Scraped Count\n",
    "        cnt = 0\n",
    "        \n",
    "        # Crawling process\n",
    "        while cnt<num_docs and not len(self.url_frontier)==0:\n",
    "            url = self.base_url + self.url_frontier[0]\n",
    "            self.url_frontier.pop(0)\n",
    "            \n",
    "            if url in scraped_links:\n",
    "                continue\n",
    "            \n",
    "            scraped_links.add(url)\n",
    "            \n",
    "            \n",
    "            file_path = self.store + \"/\" + str(cnt) + \".txt\"\n",
    "            file = open(file_path, 'w')\n",
    "            \n",
    "            page = requests.get(url)\n",
    "            \n",
    "            content,links = self._parse(page)\n",
    "            \n",
    "            self._add_links(links)\n",
    "\n",
    "            \n",
    "            file.write(self._to_text(content))\n",
    "            file.close()\n",
    "            \n",
    "            cnt += 1\n",
    "        \n",
    "    def _to_text(self,content):\n",
    "        text = \"Title : \"+ content['title']\n",
    "        text += \"\\n\\n\"\n",
    "        text += \"\\n\".join(content[\"content\"])\n",
    "        return text\n",
    "    \n",
    "    def _parse(self,page):\n",
    "        soup = bs(page.content, 'html.parser')\n",
    "        \n",
    "        body = soup.find(id=\"mw-content-text\").find('div')\n",
    "        \n",
    "        title = soup.find(id=\"firstHeading\")\n",
    "        table_of_contents = soup.find(id=\"toc\")\n",
    "        content = body.find_all(['h2','h3', 'h4', 'p'])\n",
    "        \n",
    "        text = {}\n",
    "        \n",
    "        text[\"title\"] = title.get_text()\n",
    "#         text[\"toc\"] = self._parse_toc(table_of_contents)\n",
    "        text[\"content\"], links = self._parse_content(content)\n",
    "        \n",
    "        return (text,links)\n",
    "    \n",
    "    def _parse_toc(self,toc):\n",
    "        raw_list = toc.get_text().split(\"\\n\")\n",
    "        tuple_list = [ t.split(\" \") for t in raw_list]\n",
    "        \n",
    "        final = []\n",
    "        for n,head in tuple_list:\n",
    "            if n.find('.') == -1:\n",
    "                final.push(head)\n",
    "                \n",
    "        return final\n",
    "                \n",
    "    \n",
    "    def _parse_content(self,content):\n",
    "        text = []\n",
    "        links = []\n",
    "        for tag in content :\n",
    "            text.append(tag.get_text())\n",
    "            links.extend(tag.find_all('a'))\n",
    "        \n",
    "        return (text,links)\n",
    "    \n",
    "    def _add_links(self,links):\n",
    "        flag = True\n",
    "        for link in links:\n",
    "            s = str(link)\n",
    "            k = s.find('>')\n",
    "            s = s[1:k].split(\" \")\n",
    "            for t in s:\n",
    "                if t.find(\"href\") != -1:\n",
    "                    url = t.split(\"=\")[1][1:-1]\n",
    "                    if url[:6] == \"/wiki/\" and url[6:].find('/') == -1:\n",
    "                        self.url_frontier.append(url)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = Crawler(\"https://en.wikipedia.org\")\n",
    "wiki.scrap(\"/wiki/Osiris\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
